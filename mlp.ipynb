{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_HIDDEN:  50\n",
      "LEARNING_RATE:  0.05\n",
      "BATCH_SIZE:  64\n",
      "NUM_EPOCH:  40\n",
      "len(trainX):  10000\n",
      "len(testX):  5000\n",
      "Shape of w: (39760,)\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch:1    Loss:1.3960728279634358    Train Acc:0.83    Test Acc:0.8118\n",
      "Epoch:2    Loss:0.5445833239454475    Train Acc:0.8711    Test Acc:0.8576\n",
      "Epoch:3    Loss:0.40233907378475225    Train Acc:0.9005    Test Acc:0.889\n",
      "Epoch:4    Loss:0.341530731752719    Train Acc:0.9069    Test Acc:0.8968\n",
      "Epoch:5    Loss:0.3056690256855643    Train Acc:0.9186    Test Acc:0.9106\n",
      "Epoch:6    Loss:0.28022023961633397    Train Acc:0.9221    Test Acc:0.9116\n",
      "Epoch:7    Loss:0.2592123217197053    Train Acc:0.9252    Test Acc:0.9072\n",
      "Epoch:8    Loss:0.2425623555156651    Train Acc:0.9356    Test Acc:0.9182\n",
      "Epoch:9    Loss:0.22609267597849703    Train Acc:0.9403    Test Acc:0.9248\n",
      "Epoch:10    Loss:0.2124629558803862    Train Acc:0.9404    Test Acc:0.9234\n",
      "Epoch:11    Loss:0.20002843931643274    Train Acc:0.9453    Test Acc:0.9268\n",
      "Epoch:12    Loss:0.19027331379670928    Train Acc:0.9497    Test Acc:0.9324\n",
      "Epoch:13    Loss:0.1800887358459037    Train Acc:0.9536    Test Acc:0.9334\n",
      "Epoch:14    Loss:0.17085988746151543    Train Acc:0.9546    Test Acc:0.9328\n",
      "Epoch:15    Loss:0.1625212704771887    Train Acc:0.9537    Test Acc:0.9338\n",
      "Epoch:16    Loss:0.1555880572517147    Train Acc:0.9589    Test Acc:0.9358\n",
      "Epoch:17    Loss:0.14893587272011596    Train Acc:0.9593    Test Acc:0.937\n",
      "Epoch:18    Loss:0.14225912733296167    Train Acc:0.9602    Test Acc:0.938\n",
      "Epoch:19    Loss:0.13665468440750675    Train Acc:0.965    Test Acc:0.9366\n",
      "Epoch:20    Loss:0.1313488055376726    Train Acc:0.9652    Test Acc:0.9376\n",
      "Epoch:21    Loss:0.1253941600169617    Train Acc:0.9669    Test Acc:0.9394\n",
      "Epoch:22    Loss:0.12100596729513587    Train Acc:0.9702    Test Acc:0.9394\n",
      "Epoch:23    Loss:0.11592240356636355    Train Acc:0.968    Test Acc:0.9392\n",
      "Epoch:24    Loss:0.11186131067842775    Train Acc:0.9703    Test Acc:0.9414\n",
      "Epoch:25    Loss:0.10795461372400596    Train Acc:0.9727    Test Acc:0.944\n",
      "Epoch:26    Loss:0.10378261397421777    Train Acc:0.9735    Test Acc:0.943\n",
      "Epoch:27    Loss:0.10043984662353317    Train Acc:0.9758    Test Acc:0.9444\n",
      "Epoch:28    Loss:0.09647162517440463    Train Acc:0.977    Test Acc:0.9436\n",
      "Epoch:29    Loss:0.09319504031649507    Train Acc:0.9763    Test Acc:0.9448\n",
      "Epoch:30    Loss:0.08985738833252943    Train Acc:0.9778    Test Acc:0.944\n",
      "Epoch:31    Loss:0.08673509096901684    Train Acc:0.9783    Test Acc:0.9442\n",
      "Epoch:32    Loss:0.08409989627655323    Train Acc:0.9784    Test Acc:0.945\n",
      "Epoch:33    Loss:0.08054803154221687    Train Acc:0.9794    Test Acc:0.9436\n",
      "Epoch:34    Loss:0.07826645940413501    Train Acc:0.9813    Test Acc:0.9436\n",
      "Epoch:35    Loss:0.07570026467387422    Train Acc:0.9829    Test Acc:0.9452\n",
      "Epoch:36    Loss:0.07308597781652312    Train Acc:0.9841    Test Acc:0.9462\n",
      "Epoch:37    Loss:0.07048500636732719    Train Acc:0.9845    Test Acc:0.947\n",
      "Epoch:38    Loss:0.06833739782724943    Train Acc:0.9849    Test Acc:0.9462\n",
      "Epoch:39    Loss:0.06558067728825641    Train Acc:0.985    Test Acc:0.9458\n",
      "Epoch:40    Loss:0.06385023183303475    Train Acc:0.9868    Test Acc:0.9472\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Network architecture\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "## Hyperparameters\n",
    "NUM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH = 40\n",
    "\n",
    "print(\"NUM_HIDDEN: \", NUM_HIDDEN)\n",
    "print(\"LEARNING_RATE: \", LEARNING_RATE)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"NUM_EPOCH: \", NUM_EPOCH)\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "def unpack (w):\n",
    "    W1 = np.reshape(w[:NUM_INPUT * NUM_HIDDEN],(NUM_INPUT,NUM_HIDDEN))\n",
    "    w = w[NUM_INPUT * NUM_HIDDEN:]\n",
    "    b1 = np.reshape(w[:NUM_HIDDEN], NUM_HIDDEN)\n",
    "    w = w[NUM_HIDDEN:]\n",
    "    W2 = np.reshape(w[:NUM_HIDDEN*NUM_OUTPUT], (NUM_HIDDEN,NUM_OUTPUT))\n",
    "    w = w[NUM_HIDDEN*NUM_OUTPUT:]\n",
    "    b2 = np.reshape(w,NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1_ = np.reshape(W1,NUM_INPUT*NUM_HIDDEN)\n",
    "    # print(W1_.shape)\n",
    "    W2_ = np.reshape(W2,NUM_HIDDEN*NUM_OUTPUT)\n",
    "    # print(W2_.shape)\n",
    "    w = np.concatenate((W1_,b1, W2_, b2))\n",
    "    # print(w.shape)\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"./data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"./data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "## 1. Forward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss.\n",
    "\n",
    "def fCE (X, Y, w):\n",
    "    # print(X.shape)\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    \n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    h1 = np.maximum(0, z1)\n",
    "    z2 = np.dot(h1, W2) + b2\n",
    "    exp_z2 = np.exp(z2)\n",
    "    exp_sum = np.sum(exp_z2, axis=1).reshape(-1, 1)\n",
    "    Y_pred = exp_z2 / exp_sum\n",
    "    \n",
    "    loss = -np.sum(np.log(Y_pred) * Y) / X.shape[0]\n",
    "    return loss\n",
    "\n",
    "## 2. Backward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. \n",
    "def gradCE (X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    \n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    h1 = np.maximum(0, z1)\n",
    "    z2 = np.dot(h1, W2) + b2\n",
    "    exp_z2 = np.exp(z2)\n",
    "    exp_sum = np.sum(exp_z2, axis=1).reshape(-1, 1)\n",
    "    Y_pred = exp_z2 / exp_sum\n",
    "    \n",
    "    delta_W_2 = np.dot(h1.T, Y_pred-Y) / X.shape[0]\n",
    "    delta_b_2 = np.sum(Y_pred-Y, axis=0) / X.shape[0]\n",
    "    delta_W_1 = X.T.dot((Y_pred-Y).dot(W2.T) * np.sign(z1)) / X.shape[0]\n",
    "    delta_b_1 = np.sum((Y_pred-Y).dot(W2.T) * np.sign(z1), axis=0) / X.shape[0]\n",
    "    \n",
    "    delta = pack(delta_W_1, delta_b_1, delta_W_2, delta_b_2)\n",
    "    return delta\n",
    "\n",
    "def calAccuracy (X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    \n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    h1 = np.maximum(0, z1)\n",
    "    z2 = np.dot(h1, W2) + b2\n",
    "    exp_z2 = np.exp(z2)\n",
    "    exp_sum = np.sum(exp_z2, axis=1).reshape(-1, 1)\n",
    "    Y_pred = exp_z2 / exp_sum\n",
    "    \n",
    "    Y_pred_agm = np.argmax(Y_pred, axis=1)\n",
    "    Y_agm = np.argmax(Y, axis=1)\n",
    "    \n",
    "    acc = 0\n",
    "    for pred, label in zip(Y_pred_agm, Y_agm):\n",
    "        if pred == label:\n",
    "            acc += 1\n",
    "    acc /= X.shape[0]\n",
    "    return acc\n",
    "\n",
    "## 3. Parameter Update\n",
    "# Given training and testing datasets and an initial set of weights/biases,\n",
    "# train the NN.\n",
    "def train(trainX, trainY, testX, testY, w):\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        loss = 0\n",
    "        permutation = np.random.permutation(trainX.shape[0])\n",
    "        trainX = trainX[permutation, :]\n",
    "        trainY = trainY[permutation]\n",
    "        for batch_begin in range(len(trainX))[::BATCH_SIZE]:\n",
    "            batch_end = min(trainX.shape[0], batch_begin+BATCH_SIZE)\n",
    "            X = trainX[batch_begin:batch_end,:]\n",
    "            Y = trainY[batch_begin:batch_end]\n",
    "            loss += fCE(X, Y, w) * (batch_end-batch_begin)\n",
    "            w -= LEARNING_RATE * gradCE(X, Y, w)\n",
    "        loss /= trainX.shape[0]\n",
    "        train_acc = calAccuracy(trainX, trainY, w)\n",
    "        test_acc = calAccuracy(testX, testY, w)\n",
    "        print(f'Epoch:{epoch+1}    Loss:{loss}    Train Acc:{train_acc}    Test Acc:{test_acc}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    start_time = time.time()\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    print(\"Shape of w:\",w.shape)\n",
    "    \n",
    "    # Train the network and report the accuracy on the training and test set.\n",
    "    train(trainX, trainY, testX, testY, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
